{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_process_caption_data()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "caption_file = '/root/DLResearch/show-attend-and-tell/data/annotations/captions_train2014.json'\n",
    "image_dir = '/root/DLResearch/show-attend-and-tell/image/train2014_resized/'\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _process_caption_data(caption_file, image_dir, max_length):\n",
    "    with open(caption_file) as f:\n",
    "        caption_data = json.load(f)\n",
    "        \n",
    "    # join the dataset\n",
    "    imagesDF = pd.DataFrame.from_dict(caption_data['images'])\n",
    "    annotationsDF = pd.DataFrame.from_dict(caption_data['annotations'])\n",
    "    imagesDF['file_name'] = imagesDF['file_name'].apply(lambda x: os.path.join(image_dir, x)) # append the directory\n",
    "    imagesDF = imagesDF[['file_name', 'id']] # remove unnecessary cols\n",
    "    captionDF = pd.merge(left=imagesDF, right=annotationsDF, left_on='id', right_on='image_id')\n",
    "    captionDF = captionDF.drop(['id_x', 'id_y'], axis=1)\n",
    "    captionDF.sort_values(by='image_id', inplace=True)\n",
    "    \n",
    "    # process texts\n",
    "    process_fn_1 = lambda x: x.replace('.','').replace(',','').replace(\"'\",\"\").replace('\"','') \\\n",
    "                              .replace('&','and').replace('(','').replace(\")\",\"\").replace('-',' ')\n",
    "    process_fn_2 = lambda x: \" \".join(process_fn_1(x).split())\n",
    "    process_fn_3 = lambda x: process_fn_2(x).lower()\n",
    "    \n",
    "    captionDF['caption'] = captionDF['caption'].apply(process_fn_3)\n",
    "    \n",
    "    # cut texts length\n",
    "    captionDF['text_length'] = captionDF['caption'].apply(lambda x: len(x.split()))\n",
    "    captionDF = captionDF[captionDF['text_length'] <= max_length]\n",
    "    captionDF = captionDF.drop(['text_length'], axis=1)\n",
    "    \n",
    "    captionDF = captionDF.reset_index(drop=True)\n",
    "    \n",
    "    return captionDF\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TEST: `_process_caption_data()`\n",
    "passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import originalPrepro\n",
    "importlib.reload(originalPrepro)\n",
    "caption_file = '/root/DLResearch/show-attend-and-tell/data/annotations/captions_train2014.json'\n",
    "image_dir = '/root/DLResearch/show-attend-and-tell/image/train2014_resized/'\n",
    "max_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of captions before deletion: 414113\n",
      "The number of captions after deletion: 3251\n",
      "CPU times: user 5.36 s, sys: 40 ms, total: 5.4 s\n",
      "Wall time: 5.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "caption_gt = originalPrepro.__process_caption_data(caption_file, image_dir, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.95 s, sys: 148 ms, total: 4.1 s\n",
      "Wall time: 4.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "caption = _process_caption_data(caption_file, image_dir, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>file_name</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a giraffe standing up nearby a tree</td>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a number of giraffes near one another</td>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the woman is standing with her luggage</td>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a woman playing a video game indoors</td>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a person on the ground wearing skis</td>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  caption  \\\n",
       "0     a giraffe standing up nearby a tree   \n",
       "1   a number of giraffes near one another   \n",
       "2  the woman is standing with her luggage   \n",
       "3    a woman playing a video game indoors   \n",
       "4     a person on the ground wearing skis   \n",
       "\n",
       "                                           file_name  image_id  \n",
       "0  /root/DLResearch/show-attend-and-tell/image/tr...        25  \n",
       "1  /root/DLResearch/show-attend-and-tell/image/tr...       144  \n",
       "2  /root/DLResearch/show-attend-and-tell/image/tr...       260  \n",
       "3  /root/DLResearch/show-attend-and-tell/image/tr...       446  \n",
       "4  /root/DLResearch/show-attend-and-tell/image/tr...       897  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>caption</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>a giraffe standing up nearby a tree</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>a number of giraffes near one another</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>the woman is standing with her luggage</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>a woman playing a video game indoors</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/root/DLResearch/show-attend-and-tell/image/tr...</td>\n",
       "      <td>a person on the ground wearing skis</td>\n",
       "      <td>897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  /root/DLResearch/show-attend-and-tell/image/tr...   \n",
       "1  /root/DLResearch/show-attend-and-tell/image/tr...   \n",
       "2  /root/DLResearch/show-attend-and-tell/image/tr...   \n",
       "3  /root/DLResearch/show-attend-and-tell/image/tr...   \n",
       "4  /root/DLResearch/show-attend-and-tell/image/tr...   \n",
       "\n",
       "                                  caption  image_id  \n",
       "0     a giraffe standing up nearby a tree        25  \n",
       "1   a number of giraffes near one another       144  \n",
       "2  the woman is standing with her luggage       260  \n",
       "3    a woman playing a video game indoors       446  \n",
       "4     a person on the ground wearing skis       897  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caption.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption.sort_values(['image_id', 'caption'], inplace=True)\n",
    "caption = caption.reset_index(drop=True)\n",
    "caption_gt.sort_values(['image_id', 'caption'], inplace=True)\n",
    "caption_gt = caption_gt.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for col in caption.columns:\n",
    "    print(caption[col].equals(caption_gt[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_build_vocab()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _build_vocab(annotations, threshold=1):\n",
    "    print('The Index will be different from the original version')\n",
    "    words = []\n",
    "    for sentence in annotations['caption']:\n",
    "        [words.append(w) for w in sentence.split()]\n",
    "    word_counts = pd.value_counts(np.array(words))\n",
    "    word_counts_filtered = word_counts[word_counts >= threshold]\n",
    "    \n",
    "    idx = 3\n",
    "    word_to_idx = {u'<NULL>': 0, u'<START>': 1, u'<END>': 2} \n",
    "    for word in word_counts_filtered.keys():\n",
    "        word_to_idx[word] = idx\n",
    "        idx += 1\n",
    "    \n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: `_build_vocab()`\n",
    "passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 2491 words to 2491 words with word count threshold 1.\n",
      "Max length of caption:  7\n",
      "The Index will be different from the original version\n",
      "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\n",
      "Wall time: 37.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import originalPrepro\n",
    "importlib.reload(originalPrepro)\n",
    "#caption_gt = originalPrepro.__process_caption_data(caption_file, image_dir, max_length)\n",
    "word_to_idx_gt = originalPrepro._build_vocab(caption_gt)\n",
    "word_to_idx = _build_vocab(caption_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert len(word_to_idx_gt) == len(word_to_idx)\n",
    "for w in word_to_idx_gt.keys():\n",
    "    assert w in word_to_idx.keys()\n",
    "for w in word_to_idx.keys():\n",
    "    assert w in word_to_idx_gt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "3\n",
      "683\n",
      "15\n",
      "167\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "test_word = ['snowboarding', 'passengers', 'kids']\n",
    "for w in test_word:\n",
    "    print(word_to_idx.get(w))\n",
    "    print(word_to_idx_gt.get(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_build_caption_vector()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_caption_vector(annotations, word_to_idx, max_length=15):\n",
    "    # append indicators\n",
    "    append_fn = lambda x: ' '.join(['<START>', x, '<END>'])\n",
    "    annotations['_caption'] = annotations['caption'].apply(append_fn)\n",
    "    # to vectors\n",
    "    word_to_idx_fn = lambda sentence: [word_to_idx.get(word) for word in sentence.split() if word in word_to_idx]\n",
    "    captions = annotations['_caption'].apply(word_to_idx_fn)\n",
    "\n",
    "    C = np.zeros([annotations.shape[0], max_length+2], dtype=np.int32)\n",
    "    for c in captions.items():\n",
    "        idx = c[0]\n",
    "        v = c[1]\n",
    "        if(len(v) < max_length + 2):\n",
    "            for _ in range(max_length + 2 - len(v)):\n",
    "                v.append(word_to_idx['<NULL>'])\n",
    "        C[idx, :] = v\n",
    "            \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: `_build_caption_vector()`\n",
    "passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished building caption vectors\n",
      "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\n",
      "Wall time: 36.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import originalPrepro\n",
    "import numpy as np\n",
    "importlib.reload(originalPrepro)\n",
    "captions_vec_gt = originalPrepro._build_caption_vector(caption_gt, word_to_idx_gt, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 30 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "captions_vec = _build_caption_vector(caption_gt, word_to_idx_gt, max_length=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(captions_vec_gt == captions_vec).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 2457, 2059, ...,    0,    0,    0],\n",
       "       [   1, 2457,  682, ...,    0,    0,    0],\n",
       "       [   1, 2220, 1708, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [   1, 1521, 1430, ...,    0,    0,    0],\n",
       "       [   1, 2319, 1521, ...,    0,    0,    0],\n",
       "       [   1,  465, 1475, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1, 2457, 2059, ...,    0,    0,    0],\n",
       "       [   1, 2457,  682, ...,    0,    0,    0],\n",
       "       [   1, 2220, 1708, ...,    0,    0,    0],\n",
       "       ..., \n",
       "       [   1, 1521, 1430, ...,    0,    0,    0],\n",
       "       [   1, 2319, 1521, ...,    0,    0,    0],\n",
       "       [   1,  465, 1475, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions_vec_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_build_file_names()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_file_names(annotations):\n",
    "    file_names = []\n",
    "    id_to_idx = {}\n",
    "    \n",
    "    idx = 0\n",
    "    for image_id in annotations['image_id'].unique():\n",
    "        id_to_idx[image_id] = idx\n",
    "        file_name_id = annotations[annotations.image_id == image_id].index[0]\n",
    "        file_names.append(annotations['file_name'][file_name_id])\n",
    "        idx += 1\n",
    "        \n",
    "    file_names = np.array(file_names)\n",
    "    return file_names, id_to_idx\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: `_build_file_names()`\n",
    "passed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.77 s, sys: 0 ns, total: 1.77 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "file_names, id_to_idx = _build_file_names(caption_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 3.57 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import originalPrepro\n",
    "import numpy as np\n",
    "importlib.reload(originalPrepro)\n",
    "file_names_gt, id_to_idx_gt = originalPrepro._build_file_names(caption_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(file_names_gt == file_names).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_idx_gt == id_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_build_image_idxs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_image_idxs(annotations, id_to_idx):\n",
    "    image_idxs = np.ndarray(len(annotations), dtype=np.int32)\n",
    "    \n",
    "    for item in annotations['image_id'].items():\n",
    "        idx = item[0]\n",
    "        id = item[1]\n",
    "        image_idxs[idx] = id_to_idx.get(id)\n",
    "    return image_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST:`_build_image_idxs()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 4.25 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import importlib\n",
    "import originalPrepro\n",
    "import numpy as np\n",
    "importlib.reload(originalPrepro)\n",
    "image_idxs_gt = originalPrepro._build_image_idxs(caption_gt, id_to_idx_gt)\n",
    "image_idxs = _build_image_idxs(caption_gt, id_to_idx_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(image_idxs == image_idxs_gt).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copied from Original Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_vgg_model_path = '/root/DLResearch/show-attend-and-tell/data/imagenet-vgg-verydeep-19.mat'\n",
    "caption_file_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/annotations/captions_%s2014.json'%split\n",
    "image_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/image/%s2014_resized/'%split\n",
    "save_annotation_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/%s/%s.annotations.pkl'%(split,split)\n",
    "save_word_to_idx_fn = '/root/DLResearch/show-attend-and-tell/data/%s/word_to_idx.pkl'%'train'\n",
    "save_captions_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/%s/%s.captions.pkl'%(split,split)\n",
    "save_file_names_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/%s/%s.file.names.pkl'%(split,split)\n",
    "save_image_idxs_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/%s/%s.image.idxs.pkl'%(split,split)\n",
    "save_feature_to_captions_dir_fn = lambda split: '/root/DLResearch/show-attend-and-tell/data/%s/%s.references.pkl'%(split,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing caption data\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/train.annotations.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/val/val.annotations.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/test/test.annotations.pkl..\n",
      "Loaded /root/DLResearch/show-attend-and-tell/data/train/train.annotations.pkl..\n",
      "Filtered 23107 words to 23107 words with word count threshold 1.\n",
      "Max length of caption:  15\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/word_to_idx.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/train.captions.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/train.file.names.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/train.image.idxs.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/train/train.references.pkl..\n",
      "Finished building train caption dataset\n",
      "Loaded /root/DLResearch/show-attend-and-tell/data/val/val.annotations.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/val/val.captions.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/val/val.file.names.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/val/val.image.idxs.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/val/val.references.pkl..\n",
      "Finished building val caption dataset\n",
      "Loaded /root/DLResearch/show-attend-and-tell/data/test/test.annotations.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/test/test.captions.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/test/test.file.names.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/test/test.image.idxs.pkl..\n",
      "Saved /root/DLResearch/show-attend-and-tell/data/test/test.references.pkl..\n",
      "Finished building test caption dataset\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/root/DLResearch/show-attend-and-tell/')\n",
    "from core.utils import *\n",
    "\n",
    "\n",
    "# batch size for extracting feature vectors from vggnet.\n",
    "batch_size = 100\n",
    "# maximum length of caption(number of word). if caption is longer than max_length, deleted.  \n",
    "max_length = 15\n",
    "# if word occurs less than word_count_threshold in training dataset, the word index is special unknown token.\n",
    "word_count_threshold = 1\n",
    "# vgg model path \n",
    "vgg_model_path = _vgg_model_path\n",
    "\n",
    "# about 80000 images and 400000 captions for train dataset\n",
    "train_dataset = _process_caption_data(caption_file=caption_file_dir_fn('train'),\n",
    "                                      image_dir=image_dir_fn('train'), \n",
    "                                      max_length=max_length)\n",
    "\n",
    "# about 40000 images and 200000 captions\n",
    "val_dataset = _process_caption_data(caption_file=caption_file_dir_fn('val'),\n",
    "                                    image_dir=image_dir_fn('val'), \n",
    "                                    max_length=max_length)\n",
    "\n",
    "# about 4000 images and 20000 captions for val / test dataset\n",
    "val_cutoff = int(0.1 * len(val_dataset))\n",
    "test_cutoff = int(0.2 * len(val_dataset))\n",
    "print ('Finished processing caption data')\n",
    "\n",
    "save_pickle(train_dataset, save_annotation_dir_fn('train'))\n",
    "save_pickle(val_dataset[:val_cutoff], save_annotation_dir_fn('val'))\n",
    "save_pickle(val_dataset[val_cutoff:test_cutoff].reset_index(drop=True), save_annotation_dir_fn('test'))\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    annotations = load_pickle(save_annotation_dir_fn(split))\n",
    "\n",
    "    if split == 'train':\n",
    "        word_to_idx = originalPrepro._build_vocab(annotations=annotations, threshold=word_count_threshold)\n",
    "        assert split is 'train'\n",
    "        save_pickle(word_to_idx, save_word_to_idx)\n",
    "\n",
    "    captions = _build_caption_vector(annotations=annotations, word_to_idx=word_to_idx, max_length=max_length)\n",
    "    save_pickle(captions, save_captions_dir_fn(split))\n",
    "\n",
    "    file_names, id_to_idx = _build_file_names(annotations)\n",
    "    save_pickle(file_names, save_file_names_dir_fn(split))\n",
    "\n",
    "    image_idxs = _build_image_idxs(annotations, id_to_idx)\n",
    "    save_pickle(image_idxs, save_image_idxs_dir_fn(split))\n",
    "\n",
    "    # prepare reference captions to compute bleu scores later\n",
    "    image_ids = {}\n",
    "    feature_to_captions = {}\n",
    "    i = -1\n",
    "    for caption, image_id in zip(annotations['caption'], annotations['image_id']):\n",
    "        if not image_id in image_ids:\n",
    "            image_ids[image_id] = 0\n",
    "            i += 1\n",
    "            feature_to_captions[i] = []\n",
    "        feature_to_captions[i].append(caption.lower() + ' .')\n",
    "    save_pickle(feature_to_captions, save_feature_to_captions_dir_fn(split))\n",
    "    print(\"Finished building %s caption dataset\" %split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `_vgg_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/root/DLResearch/show-attend-and-tell/')\n",
    "from core.vggnet import Vgg19\n",
    "import tensorflow as tf\n",
    "\n",
    "def _vgg_features(vgg_path):\n",
    "    vggnet = Vgg19(vgg_path)\n",
    "    vggnet.build()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.29 s, sys: 612 ms, total: 9.9 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "_vgg_model_path = '/root/DLResearch/show-attend-and-tell/data/imagenet-vgg-verydeep-19.mat'\n",
    "vggnet = Vgg19(_vgg_model_path)\n",
    "vggnet.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno_path = save_annotation_dir_fn('train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
